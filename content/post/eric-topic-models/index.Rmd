---
title: Topic Modeling Academic Articles on K-12 Student Achievement
summary: Using Structural Topic Modeling in R
author: "Akram Al-Turk"
date: "8/5/2019"
output: html_document
draft: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd('C:\\Users\\Akram Al-Turk\\OneDrive - University of North Carolina at Chapel Hill\\Research\\Dissertation')
```

Topic modeling is an inductively driven text analytic method commonly used to show the underlying, latent topics of large bodies of text (Blei, Ng, and Jordan 2003; McFarland et al. 2013; Mohr and Bogdanov 2013). While researchers often use topic modeling for descriptive purposes, recent extensions allow a researcher to predict the likelihood that a documentâ€™s metadata (e.g., publication year, author, funder, etc.) is correlated with the proportion of that document dedicated to a particular topic (Roberts, Stewart, and Tingley n.d.). In short, I run topic models using the abstracts of ERIC articles and run a regression that estimates the effect of federal sponsorship on the proportion of each article that is about each of the latent topics.

```{r libraries, echo=FALSE}
library(stm)
library(tidyverse)
library(furrr)
library(plotly)
library(htmlwidgets)
```

```{r, echo=FALSE}
achieve_orig <- read_csv('achieve_articles_60_94.csv')

ach_inst_dummies <- read_csv('achieve_60_94_inst_dummies.csv')

achieve_pre_83_dummies = read_csv('ach_pre_83_dummies.csv')

achieve_orig_inst <- merge(achieve_orig, ach_inst_dummies, by = 'accno')

# just select last columns, since they're already in achieve_orig
achieve_pre_83_dummies <- achieve_pre_83_dummies %>% 
  select(16:24)

achieve = merge(achieve_orig_inst, achieve_pre_83_dummies, by = 'accno')
```

```{r}
num_k <- c(8, 10, 12, 14, 16, 18, 20)

stop_words <- c('education', 'educational', 'achievement', 'achieve', 'quot', 'author', 'authors', 'review', 'study', 'academic', 'studies', 'research', 'paper', 'document', 'report', 'use', 'can', 'may', 'documents', 'will', 'must')
```

```{r, include=FALSE}
load('achieve_multiple_k_diagnostics.RData')
```

```{r}
achieve_process <- textProcessor(achieve$description, metadata = achieve, onlycharacter = TRUE, stem = FALSE, customstopwords = stop_words)

# drop words that appear in fewer than 5 documents and more than 70 percent of documents (lower.thresh default is fewer than 2)
achieve_out <- prepDocuments(achieve_process$documents, achieve_process$vocab, achieve_process$meta, lower.thresh = 4, upper.thresh = floor(nrow(achieve)*.7))
```

A common critique of topic modeling is that researchers have to choose the number of latent topics for the body of texts, an often subjective and difficult to replicate process. I address these concerns in the appendix, but in short, I run models that include different numbers of topics. For each model, I run a regression that estimates the effect of sponsorship by education agencies on the proportion of each article that is about each of the topics in the model. For these models, I control, similarly to the logistic regression above, for publication year and whether an article was funded by a private foundation.

```{r}
plan(multiprocess)

many_models <- tibble(K = c(8, 10, 12, 14, 16, 18, 20)) %>%
  mutate(topic_model = future_map(K, ~stm(documents = achieve_out$documents, vocab = achieve_out$vocab, data = achieve_out$meta, K = ., prevalence = ~ s(pub_year) + sponsor_dummy, max.em.its = 75, seed = 1234, verbose = TRUE)))
```

```{r}
heldout <- make.heldout(documents = achieve_out$documents, vocab = achieve_out$vocab)

k_result <- many_models %>%
  mutate(exclusivity = map(topic_model, exclusivity),
         semantic_coherence = map(topic_model, semanticCoherence, achieve_out$documents),
         eval_heldout = map(topic_model, eval.heldout, heldout$missing),
         residual = map(topic_model, checkResiduals, achieve_out$documents),
         bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
         lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
         lbound = bound + lfact,
         iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))

k_fig1 <- k_result %>%
  transmute(K,
            `Lower bound` = lbound,
            Residuals = map_dbl(residual, "dispersion"),
            `Semantic coherence` = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Model diagnostics by number of topics")

ggsave("fig1.png", k_fig1, height = 10, width = 16, units = "in")
```

```{r}
k_result %>%
  transmute(K,
            `Lower bound` = lbound,
            Residuals = map_dbl(residual, "dispersion"),
            `Semantic coherence` = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Model diagnostics by number of topics")

k_result %>%
  select(K, exclusivity, semantic_coherence) %>%
  filter(K %in% c(10, 12, 14)) %>%
  unnest() %>%
  mutate(K = as.factor(K)) %>%
  ggplot(aes(semantic_coherence, exclusivity, color = K)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = "Comparing exclusivity and semantic coherence",
       subtitle = "Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity")
```

```{r}
# map(many_models$topic_model, labelTopics)
```

```{r}
# save.image('achieve_multiple_k_diagnostics.RData')
```